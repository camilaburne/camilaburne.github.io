I"∂'<p>This post follows a <a href="https://www.kaggle.com/statsfromarg/btc-blockchain-data-exploratory-analysis">notebook I made on Kaggle</a> as a first exploratory approach to understanding Bitcoin blockchain data. The goal of this EDA is to become familiarized with the blockchain data structure, and where to find information about blocks, transactions and addresses.</p>

<p>Bitcoin blockchain data is stored in two main tables - blocks and transactions, within transactions there are two nested tables, inputs and outputs.</p>

<p>To start exploring blockchain data I extracted the first rows of all four tables, mapped each field with its meaning, and made some aggregates to get insights.</p>

<ul>
  <li><a href="#blocks"><strong>Blocks</strong></a>, I visualized number and size of blocks by month, since 2009.</li>
  <li><a href="#txns"><strong>Transactions</strong></a> I analyze one transaction to understand how transactions, inputs and outputs are linked (Pending to do)</li>
  <li><a href="#outputs"><strong>Outputs</strong></a> Extract the current number of unspent btc. (Pending to do)</li>
  <li><a href="#inputs"><strong>Inputs</strong></a> Visualize addresses through time. (Pending to do)</li>
</ul>

<p>Resources:</p>
<ul>
  <li><a href="https://github.com/bitcoinbook/bitcoinbook">Mastering Bitcoin (book on github)</a></li>
  <li><a href="https://github.com/blockchain-etl">Bitcoin ETL repo, the one that‚Äôs dumping btc data on bigquery</a></li>
  <li><a href="https://towardsdatascience.com/https-medium-com-nocibambi-getting-started-with-bitcoin-data-on-kaggle-with-python-and-bigquery-d5266aa9f52b">Getting started with Bitcoin data on Kaggle with Python and BigQuery</a></li>
</ul>

<p><br /></p>

<hr />
<h3 id="blocks-">Blocks <a name="blocks"></a></h3>

<p>Blocks table contains (‚Ä¶).
Field descriptions <a href="https://github.com/blockchain-etl/bitcoin-etl-airflow/blob/master/dags/resources/stages/enrich/schemas/blocks.json">from the bitcoin-etl-airflow repo</a>.</p>

<ul>
  <li><strong>hash</strong>: Hash of this block</li>
  <li><strong>size</strong>: The size of block data in bytes</li>
  <li><strong>stripped_size</strong>: The size of block data in bytes excluding witness data</li>
  <li><strong>weight</strong>: Three times the base size plus the total size. <a href="https://github.com/bitcoin/bips/blob/master/bip-0141.mediawiki">More info.</a></li>
  <li><strong>number</strong>: The number of the block</li>
  <li><strong>version</strong>: Protocol version specified in block header</li>
  <li><strong>merkle_root</strong>: The root node of a Merkle tree, where leaves are transaction hashes</li>
  <li><strong>timestamp</strong>: Block creation timestamp specified in block header</li>
  <li><strong>timestamp_month</strong>: Month of Block creation timestamp specified in block header</li>
  <li><strong>nonce</strong>: Difficulty solution specified in block header</li>
  <li><strong>bits</strong>: Difficulty threshold specified in block header</li>
  <li><strong>coinbase_param</strong>: Data specified in the coinbase transaction of this block</li>
  <li><strong>transaction_count</strong>: Number of transactions included in this block</li>
</ul>

<pre>
# Exploring bigquery-public-data.crypto_bitcoin.blocks blocks content

q_blocks ='''SELECT
              *       
             FROM   `bigquery-public-data.crypto_bitcoin.blocks`  
             order by timestamp_month
             limit 10
          '''

blocks = client.query(q_blocks).to_dataframe()
blocks.to_csv('blocks_head.csv')
blocks.head()

</pre>

<pre>

# Aggregating block size and count over months

q_blocks_m ='''SELECT
              timestamp_month
            , count(*)   as n_blocks
            , avg(size)  as mean_size
            , avg(stripped_size)  as mean_stripped_size        
           FROM   `bigquery-public-data.crypto_bitcoin.blocks`  
           group by 1
        '''

blocks_m = client.query(q_blocks_m).to_dataframe()
blocks_m.to_csv('blocks_size_month.csv')
blocks_m.head()
</pre>

<pre>
blocks_m['month'] = blocks_m.timestamp_month.astype(str).str[0:7]
blocks_m = blocks_m.sort_values(by=['month']).copy()
</pre>

<pre>
p = sns.barplot(x = blocks_m['month'], y= blocks_m['n_blocks'], color='teal')
p.set_title('Number of blocks by month', size = 20)
p.set_xticklabels(p.get_xticklabels(), rotation=90, size = 9);

p = sns.barplot(x = blocks_m['month'], y= blocks_m['mean_stripped_size'], color='teal')
p.set_title('Average block stripped size by month', size = 20)
p.set_xticklabels(p.get_xticklabels(), rotation=90, size = 9);
</pre>

<p><strong>Notes on column oriented and row oriented databases</strong>:</p>

<p>Columnar-store is the opposite of more traditional row oriented data stores. In row-store, information is saved and retrieved one row at a time, they are easy to read and write, but it‚Äôs not easy to aggregate or fetch all rows at the same time. This is the typical case of a ‚Äústudents database‚Äù in which we may want to see the data of a particular individual, like in many SQL tutorials.</p>

<p>Columnar oriented data store is typically used for OLAP (online analytical processing) applications. Here we would be mostly interested in performing calculations over fields, for example, fetching the amount of multiple transactions and calculating the mean.</p>

<p>Ultimately, in both situation a table looks exactly the same, but its performance is different depending on the types of query we ran against it. For row-oriented storage, we could easily see all the info for a given unit, and in column-oriented storage, we can process aggregates over columns very fast. <a href="https://medium.com/bluecore-engineering/deciding-between-row-and-columnar-stores-why-we-chose-both-3a675dab4087#:~:text=In%20row%20oriented%20databases%2C%20these,data%20is%20read%20at%20once.">Further reading.</a></p>

<p><br /></p>

<h3 id="bugs-in-cartesian-joins-">Bugs in Cartesian Joins <a name="t2"></a></h3>
<p>Cartersian joins, aka cross joins, return all the possible combinations from the records of two tables, so if you are cross joining two tables with N rows, you‚Äôll get NxN results. Unless you are designing an experiment, or building a view or another table from scratch, you will never use this join. In some SQL editors, this is the default join when you don‚Äôt specify otherwise, so always remember to write down either: <code class="language-plaintext highlighter-rouge">INNER JOIN</code> or <code class="language-plaintext highlighter-rouge">LEFT JOIN</code>. There‚Äôs no need for any other, not even right joins, that‚Äôs only used by weird people.</p>

<p>‚õîÔ∏è</p>
<pre>
SELECT
 t1.col1,
 t2.col2
FROM table1 as t1, table2 as t2 ;
</pre>

<p>‚úÖ</p>
<pre>
SELECT
 t1.col1,
 t2.col2
 FROM table1 as t1
 LEFT JOIN table2 as t2 ON t1.id = t2.t1_id ;
</pre>

<p><br /></p>

<h3 id="avoid-joins-on-big-tables-">Avoid Joins on big tables <a name="t3"></a></h3>

<p>Talking about joins: don‚Äôt join big tables with one another. It‚Äôs better to do a two step process: first reduce each original table through a sub-query, then join these subqueries. Although it looks more complex to have nested queries, it will save time since the runtime will decrease significantly.</p>

<p>Instead of:</p>

<p>‚õîÔ∏è</p>
<pre>SELECT
  c.id,
  c.age,
  t.amt

FROM customers c                          
LEFT JOIN transactions t on c.id = t.cust_id
</pre>

<p>‚úÖ</p>
<pre>SELECT *    

FROM (
      SELECT id, age
      FROM customers
      ) c        
LEFT JOIN (
      SELECT cust_id, amt
      FROM transactions) t on c.id = t.cust_id
</pre>

<h3 id="distinct-is-expensive">DISTINCT is expensive<a name="t4"></a></h3>

<p>The distinct clause is esentially a group by, but an expensive one. It is used to obtain unique information, usually when there are duplicates in your original table or in any of the joins. <code class="language-plaintext highlighter-rouge">DISTINCT</code> works by first sorting all the data, then grouping by all the fields included in the SELECT statement.</p>

<p>If you are only retrieving an index column, distinct is not a bad idea. But to use <code class="language-plaintext highlighter-rouge">DISTINCT</code> over multiple columns that are not indexes, is costly and could be avoided by other means: such as, use temporary tables with pre-aggregates, avoid certain joins.</p>

<p><br /></p>

<h3 id="with-big-tables">WITH big tables<a name="t5"></a></h3>

<p>The with clause allows you to name a subquery so you can use it in a main query. This could be useful to structure your code, when you are nesting queries, but it can be expensive as it‚Äôs running all your queries at once.</p>

<p>Querying many full, big tables, at the same time is a bad idea. Sometimes when I have to create views or tables that involve many joins, like over 10, it‚Äôs better to create intermediate temporary tables.</p>

<p>For ex, let‚Äôs imagine we have a customers table, a transactions table, and a city table. We want to have the average amount of the maximum purchase made by a customer in each city.</p>

<p>Instead of running two queries at the same time:</p>

<p>‚õîÔ∏è</p>
<pre>
WITH my_big_table
  as (SELECT
      c.id,
      c.age,
      a.city,  
      max(t.amt) as max_amt      
      FROM customers c                          
      LEFT JOIN transactions t on c.id = t.cust_id
      LEFT JOIN cust_address a on c.id = a.cust_id
      group by 1,2,3
      )  

  SELECT a.city, avg(t.amt)
  group by 1 ;

</pre>

<p>Create an intermediate temporary one, and do it two steps:</p>

<p>‚úÖ</p>
<pre>
# Exploring bigquery-public-data.crypto_bitcoin.blocks blocks content

q_blocks ='''SELECT
              *       
             FROM   `bigquery-public-data.crypto_bitcoin.blocks`  
             order by timestamp_month
             limit 10
          '''

blocks = client.query(q_blocks).to_dataframe()
blocks.to_csv('blocks_head.csv')
blocks.head()

</pre>

<p>I received these tips from my manager after she identified queries that took minutes to run. As a rule of thumb, if a query is taking over 30s, go back and find if there are any of these mistakes in your SQL.</p>
:ET