I"?<p>Anomaly or outlier detection is the search of atypical observations that don‚Äôt seem to fit with a pattern found in the majority of data.</p>

<p>There are at least two broad approaches to search for  these rare observations, one is based on defining a pattern first, and then ruling out the specific cases that don‚Äôt conform to that pattern.</p>

<p>The other approach, the one I‚Äôm interesting in using, is not concerned with the pattern of the majority of the data, but with finding ways to <em>isolate</em> observations. The idea of an isolation tree is to build a tree following the same splits as a common classification tree. When the tree is fully grown, reaching one observation in each leaf, we can then measure how long it took for the tree to reach that one observation.</p>

<p>Looking at the following chart, we can see that adjusting a tree with all leaves of size = 1 will provide different number of splits for different observations. The pink path is the shortest among all leaves, and thus can be identified as an anomaly. The other two blue leaves have 4 splits, and as the trees continues, we can see that for the rest of the observations the path length is at least 5.</p>

<p><br />
<img src="/images/isolation_tree.png" class="center" height="480px" />
<br /></p>

<p>The shorter the path between the origin and a leaf, the more rare that observation is. A measure for the path distance is the <em>number of splits needed</em> to isolate a single observation.</p>

<hr />

<p>Finding anomalies is of great interest in OLAP data, so I adjusted my Isolation Tree using an <a href="https://www.kaggle.com/arjunbhasin2013/ccdata">open kaggle data set</a> of credit card transactions. The data contains usage behavior of about 9000 active credit card holders during the last 6 months. After running the isolation tree, most detected outliers include high amounts of payments and purchases.</p>

<p>The library used to run my Isolation Tree is my favorite <a href="https://www.h2o.ai/">h20.ai üíõ</a>, but this algorithm is also <a href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.IsolationForest.html">available at scikitlearn</a>. I like h20.ai because it‚Äôs an open source, distributed, fast, and flexible machine learning platform that runs on <strong>both R and Python</strong>, which is a great relief for statisticians still undecided on which tool is best.</p>

<p>For a quick, 60ish line script, R does an amazing job</p>

<pre>
library(h2o)

options(scipen=999)

# Read file from Kaggle Open Data
#   File desc: The sample Dataset summarizes the usage behavior of about 9000
#   active credit card holders during the last 6 months. The file is at a customer
#   level with 18 behavioral variables.

cc &lt;- read.csv('CC GENERAL.csv', dec = '.')

vq &lt;-c('BALANCE','CREDIT_LIMIT','PURCHASES','PURCHASES_TRX','ONEOFF_PURCHASES',
       'INSTALLMENTS_PURCHASES','CASH_ADVANCE', 'CASH_ADVANCE_TRX')

# Duplicates
sum(duplicated(cc$CUST_ID))

# Distribution
for (i in vq) {
  hist(cc[,i], col='grey75', border='grey65',
       xlab = paste(i, 'NA count:', sum(is.na(cc[,i]))),
       main = paste(gsub('_', ' ', i), 'Median:', round(median(cc[,i], na.rm=T),1)))
  }

# Forget NAs
cc &lt;- cc[is.na(cc$CREDIT_LIMIT)==F,]
cc &lt;- cc[is.na(cc$MINIMUM_PAYMENTS)==F,]

# Use h2o object
h2o.init()
df  &lt;- as.h2o(cc[,vq])

# Build Isolation Tree
it  &lt;- h2o.isolationForest(training_frame = df,
                             sample_rate = 0.1,
                             max_depth = 20,
                             ntrees = 1000)
it

# Keep prediction and mean length and add to original df
pred   &lt;- h2o.predict(it, df)
cc_df &lt;- cbind(cc,as.data.frame(pred))

# Distribution of mean length and predicted values:
hist(cc_df$mean_length, col='grey75', border='grey65', main = 'iTree mean split length', xlab = 'Mean Split Length')
hist(cc_df$predict,     col='grey75', border='grey65', main = 'iTree predicted values', xlab = 'Predicted')
abline(v = 0.85, col='orange', lwd=3, lty=3)

# &lt;4% fall in this group
prop.table(table(cc_df$predict&gt;0.85))
table(cc_df$predict&gt;0.85)

# Scatterpots
# some more vars to plot (log) and remove the freq vars, all into vq2
cc_df$LOG_PAYMENTS &lt;- log(cc_df$PAYMENTS)
cc_df$LOG_MINIMUM_PAYMENTS &lt;- log(cc_df$MINIMUM_PAYMENTS)
vq2 &lt;- c(vq,'LOG_PAYMENTS','LOG_MINIMUM_PAYMENTS')


for (i in vq2) {
  plot(cc_df[,i], cc_df[, 'predict'], col='grey75', main=paste('iTree prediction vs', tolower(i)),
       xlab = paste(i, 'Corr =', round(cor(cc_df[,i], cc_df[, 'predict']),2)), ylab='Predicted' )
  abline(h = 0.85, col='orange', lwd=3, lty=3)
}
</pre>

<p>My <a href="https://github.com/camilaburne/h2o">repo of h2o.ai stuff I like is on github</a>, with the isolation tree case included.</p>

<p><br /></p>

<p>Resources on isolation forest:</p>
<ul>
  <li>
    <p><a href="https://cs.nju.edu.cn/zhouzh/zhouzh.files/publication/icdm08b.pdf">Published paper by: Liu, Fei Tony, Ting, Kai Ming, and Zhou, Zhi-Hua, ‚ÄúIsolation Forest‚Äù</a></p>
  </li>
  <li>
    <p><a href="https://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-science/if.html">h2o.ai documentation on Isolation Forest</a></p>
  </li>
  <li>
    <p><a href="https://scikit-learn.org/stable/auto_examples/ensemble/plot_isolation_forest.html#sphx-glr-auto-examples-ensemble-plot-isolation-forest-py">scikit documentation on Isolation Forest</a></p>
  </li>
</ul>
:ET